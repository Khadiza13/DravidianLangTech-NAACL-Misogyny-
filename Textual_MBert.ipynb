{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 10404788,
          "sourceType": "datasetVersion",
          "datasetId": 6447553
        },
        {
          "sourceId": 10404789,
          "sourceType": "datasetVersion",
          "datasetId": 6447554
        },
        {
          "sourceId": 10404793,
          "sourceType": "datasetVersion",
          "datasetId": 6447557
        },
        {
          "sourceId": 10590024,
          "sourceType": "datasetVersion",
          "datasetId": 6554085
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Textual_MBert",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khadiza13/DravidianLangTech-NAACL-Misogyny-/blob/main/Textual_MBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "sZ4W8Du_V0xj"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "khadiza13_train_dataset_path = kagglehub.dataset_download('khadiza13/train-dataset')\n",
        "khadiza13_eval_dataset_path = kagglehub.dataset_download('khadiza13/eval-dataset')\n",
        "khadiza13_test_dataset_path = kagglehub.dataset_download('khadiza13/test-dataset')\n",
        "khadiza13_test_with_labels_path = kagglehub.dataset_download('khadiza13/test-with-labels')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ZX04A1f6V0xl"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T06:13:45.452649Z",
          "iopub.execute_input": "2025-03-07T06:13:45.452959Z",
          "iopub.status.idle": "2025-03-07T06:13:46.406041Z",
          "shell.execute_reply.started": "2025-03-07T06:13:45.452936Z",
          "shell.execute_reply": "2025-03-07T06:13:46.405125Z"
        },
        "id": "nPcdULUPV0xl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_and_preprocess_data(csv_path):\n",
        "    data = pd.read_csv(csv_path)\n",
        "    texts = data['transcriptions'].tolist()\n",
        "    labels = data['labels'].tolist()\n",
        "    return texts, labels\n",
        "\n",
        "# Paths for your dataset\n",
        "TRAIN_CSV_PATH = '/kaggle/input/train-dataset/train/train.csv'\n",
        "EVAL_CSV_PATH = '/kaggle/input/eval-dataset/dev/dev.csv'\n",
        "TEST_CSV_PATH = '/kaggle/input/test-with-labels/test_with_labels/test_with_labels.csv'\n",
        "\n",
        "# Load data\n",
        "train_texts, train_labels = load_and_preprocess_data(TRAIN_CSV_PATH)\n",
        "eval_texts, eval_labels = load_and_preprocess_data(EVAL_CSV_PATH)\n",
        "test_texts, test_labels = load_and_preprocess_data(TEST_CSV_PATH)\n",
        "\n",
        "# Merge training and evaluation data\n",
        "merged_texts = train_texts + eval_texts\n",
        "merged_labels = train_labels + eval_labels\n",
        "\n",
        "# Initialize tokenizer\n",
        "text_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    model_max_length=128,\n",
        "    use_fast=True\n",
        ")\n",
        "\n",
        "# Custom classification model using mBERT\n",
        "class MBertClassifier(nn.Module):\n",
        "    def __init__(self, num_labels=2):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "        self.bert.resize_token_embeddings(len(text_tokenizer))\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]  # Take [CLS] token output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        return {'loss': loss, 'logits': logits} if loss is not None else {'logits': logits}\n",
        "\n",
        "# Initialize the custom model\n",
        "text_model = MBertClassifier(num_labels=2)\n",
        "\n",
        "def tokenize_texts(texts, tokenizer, max_length=128):\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Tokenize the data\n",
        "merged_encodings = tokenize_texts(merged_texts, text_tokenizer)\n",
        "test_encodings = tokenize_texts(test_texts, text_tokenizer)\n",
        "\n",
        "# Create datasets\n",
        "train_labels_tensor = torch.tensor(merged_labels)\n",
        "test_labels_tensor = torch.tensor(test_labels)\n",
        "\n",
        "train_dataset = TensorDataset(\n",
        "    merged_encodings['input_ids'],\n",
        "    merged_encodings['attention_mask'],\n",
        "    train_labels_tensor\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(text_model.parameters(), lr=2e-5)\n",
        "\n",
        "# Training loop with progress bar\n",
        "def train_model(model, dataloader, optimizer, epochs):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs['loss']\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Average loss for epoch {epoch+1}: {avg_loss:.4f}\")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, encodings, labels_tensor):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=encodings['input_ids'],\n",
        "            attention_mask=encodings['attention_mask']\n",
        "        )\n",
        "        logits = outputs['logits']\n",
        "        predicted_labels = torch.argmax(logits, axis=1)\n",
        "        return predicted_labels\n",
        "\n",
        "# Train the model for 5 epochs\n",
        "print(\"Starting training...\")\n",
        "train_model(text_model, train_dataloader, optimizer, epochs=5)\n",
        "\n",
        "# Evaluate on test data\n",
        "print(\"\\nEvaluating on test data...\")\n",
        "predicted_labels = evaluate_model(text_model, test_encodings, test_labels_tensor)\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(\n",
        "    test_labels_tensor,\n",
        "    predicted_labels.numpy(),\n",
        "    target_names=[\"Non-Misogyny\", \"Misogyny\"]\n",
        "))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T06:14:14.657098Z",
          "iopub.execute_input": "2025-03-07T06:14:14.657389Z",
          "iopub.status.idle": "2025-03-07T06:38:03.443658Z",
          "shell.execute_reply.started": "2025-03-07T06:14:14.657368Z",
          "shell.execute_reply": "2025-03-07T06:38:03.442812Z"
        },
        "id": "FW7P9X31V0xl",
        "outputId": "c1562a56-f497-47fd-cb13-db74c4b27e06",
        "colab": {
          "referenced_widgets": [
            "f1190525c86144f19e473f2f0e047fbf",
            "01f3a4fce23e4fec8b5f610db1c6a8a5",
            "c92c6d6fa4d14dd69257e6ec8ec8138d",
            "5ac0d786f7ba4ab8a51acf54207da1f1",
            "7a9848526340481d9547ab0590e31b89"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1190525c86144f19e473f2f0e047fbf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01f3a4fce23e4fec8b5f610db1c6a8a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c92c6d6fa4d14dd69257e6ec8ec8138d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ac0d786f7ba4ab8a51acf54207da1f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a9848526340481d9547ab0590e31b89"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Starting training...\n\nEpoch 1/5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Training: 100%|██████████| 25/25 [04:43<00:00, 11.33s/it, loss=0.7180]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Average loss for epoch 1: 0.6494\n\nEpoch 2/5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Training: 100%|██████████| 25/25 [04:33<00:00, 10.95s/it, loss=0.6197]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Average loss for epoch 2: 0.5181\n\nEpoch 3/5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Training: 100%|██████████| 25/25 [04:34<00:00, 11.00s/it, loss=0.3743]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Average loss for epoch 3: 0.4465\n\nEpoch 4/5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Training: 100%|██████████| 25/25 [04:35<00:00, 11.01s/it, loss=0.2579]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Average loss for epoch 4: 0.3226\n\nEpoch 5/5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Training: 100%|██████████| 25/25 [04:33<00:00, 10.95s/it, loss=0.1313]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Average loss for epoch 5: 0.2071\n\nEvaluating on test data...\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\nNon-Misogyny       0.76      0.84      0.79       122\n    Misogyny       0.69      0.58      0.63        78\n\n    accuracy                           0.73       200\n   macro avg       0.72      0.71      0.71       200\nweighted avg       0.73      0.73      0.73       200\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}